#+TITLE: Designing Data-Intensive Applications
#+AUTHOR: chrischen
#+OPTIONS: H:3 toc:2 num:2 ^:{}
* Overviews
  Many applications today are data-intensive, as opposed to compute-intensive.

** Functionalities
   - Store data (databases)
   - Remember the result of an expensive operation (caches)
   - Allow users to search data by filter (search indexes)
   - Send a message to another process, to be handled asynchronously (stream processing)
   - Periodically crunch a large amount of accumulated data (batch processing)

** Questions
   - How do you ensure that the data remains correct and complete?
   - How do you provide consistently good performance to clients?
   - How do you scale to handle an increase in load?
   - What does a good API for the service look like?

** Factors Influence the Design
   - Skills and experience of the people involved
   - Legacy system dependencies
   - The timescale for delivery
   - *Organization's tolerance of different kinds of risk*
   - Regulatory constraints

** Reliability
   The system should continue to work correctly (performing the correct function at
   the desired level of performance) even in the face of adversity (hardware or
   software faults, and even human error).

*** Fault vs. Failure
    - Fault :: Fault is defined as one component of the system deviating from its spec.
    - Failure :: Failure is when the system as a whole stops providing the required service to the user.

*** Hardware Faults
    - Add redundancy to reduce the failure rate.
    - A single-server system requires planned downtime if you need to reboot the machine
      (to apply operating system security patches, for example).
    - Multi-machine redundancy was only required by a small number of applications for
      which high availability was absolutely essential.
    - Hardware faults are thought of random and independent from each other.
    - Tolerate the loss of entire machines, by using software *fault-tolerance techniques in preference*.

*** Software Errors
    - Systematic errors tend to cause many more system failures than uncorrelated hardware faults.
    - Examples of systematic errors:
      - A software bug that causes every instance of an application server to crash when given a particular bad input.
      - A runaway process that uses up some shared resource.
      - A service that the system depends on that slows down, becomes unresponsive, or starts returning corrupted responses.
      - Cascading failures, where a small fault in one component triggers a fault in another component, which in turn triggers
        further faults.
    - Solutions
      - Carefully thinking about assumptions and interactions in the system
      - *Thorough Testing*
      - Process Isolation
      - Allowing processes to crash and restart
      - Measuring, monitoring, and analyzing system behavior in production

*** Human Errors
    Approaches to deal with unreliable humans
    - Well-designed abstractions, APIs and *admin interfaces*
    - Decouple the places where people make the most mistakes from the places where they can cause failures.
    - Sandbox environments to help explore and experiment safely using real data.
    - Test thoroughly at all levels, from unit tests to whole-system integration tests and manual tests.
    - Allow quick and easy recovery from human errors.
    - Set up detailed and clear monitoring, such as performance metrics and error rates.
    - Implement good management practices and training

** Scalability
   As the system grows (in data volume, traffic volume, or complexity), there
   should be reasonable ways of dealing with that growth.

*** Describing Load
    Load parameter Example:
    - Requests per second
    - The ratio of reads to writes in a database
    - The hit rate on a cache
    - The number of simultaneously active users in a chat room
    - Twitter: The distribution of followers per user (maybe weighted by how often those users tweet) is a key load parameter
      for discussing scalability, since it determines the fan-out load.

*** Describing Performance
**** Look at it in two ways
     - When you increase a load parameter and keep the system resources unchanged, how is the performance of your system affected?
     - When you increase a load parameter, how much do you need to increase the resources if you want to keep performance unchanged?

**** Performance Examples
     - Throughput :: The number of records we can process per second, or the total time it takes to run a job on a dataset of a certain size. (Important for batch processing system)
     - Response Time :: The time between a client sending a request and receiving a response. (Important for online systems)
     - Latency :: Latency is the duration that a request is waiting to be handled—during which it is latent, awaiting service.
       - The causes of outliers: context switch, the loss of a network packet, a garbage collection pause, a page fault forcing a read from disk, etc.
       - Amazon describes response time requirements for internal services in terms of the 99.9th percentile,
         even though it only affects 1 in 1,000 requests. This is because the customers with the slowest requests
         are often those who have the most data on their accounts
       - Reducing response times at very high percentiles is difficult because they are easily affected by random
         events outside of your control, and the benefits are diminishing.

*** Coping with Load
    Recommended Practices
    - keep your database on a single node (scale up) until scaling cost or high-availability
      requirements forced you to make it distributed.
    - An architecture that scales well for a particular application is built around assumptions
      of which operations will be common and which will be rare—the load parameters.


** Maintainability
   Over time, many different people will work on the system (engineering and operations,
   both maintaining current behavior and adapting the system to new use cases), and
   they should all be able to work on it productively.

*** Operability
    Make it easy for operations teams to keep the system running smoothly.
    - Monitoring the health of the system and quickly restoring service if it goes into a bad state
    - Tracking down the cause of problems
    - Keeping software and platforms up to date, including security patches
    - Establishing good practices and tools for deployment, configuration management, and more
    - Defining processes that make operations predictable and help keep the production environment stable
    - Preserving the organization's knowledge about the system, even as individual people come and go

*** Simplicity
    Make it easy for new engineers to understand the system, by removing as much complexity as possible from the system.
    - Complexities
      - Explosion of the state space
      - Tight coupling of modules
      - Tangled dependencies
      - Inconsistent naming and terminology
      - etc...

*** Evolvability
    Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change.

** A Simple Architecture
   [[../resources/DDIA/simple_data_system_architecture.png]]


* Data Models
  - The advantage of using an *ID* is that because it has no meaning to humans, it *never needs to change*.

** Relational Model
   - The roots of relational databases lie in business data processing.
   - Schema-on-write (where the schema is explicit and the database ensures all written data conforms to it)

** Document Model
   - Support *self-contained* document
   - Many-to-one and many-to-many relationships don't fit nicely into document model
   - Better performance due to *locality*
   - The application need to do additional work to keep the denormalized data consistent and emulate the join operation.
   - Schema-on-read (the structure of the data is implicit, and only interpreted when the data is read)

*** When to Use
    - There are many different types of objects, and it is not practical to put each type of object in its own table.
    - The structure of the data is determined by external systems over which you have no control and which may change at any time.
    - When your application has mostly one-to-many relationships (tree-structured data) or no relationships between records

** Graph-Like Data Models
   - Graphs are good for *evolvability*.
   - Property Graph Model (Neo4j, Titan and InfiniteGraph)
   - Triple-Store Model (Datomic, AllegroGraph)
   - Query Lanaguages (Cypher, SPARQL, and Datalog)

** Other Models
   - GenBank for genome data
   - Large Hadron Collider (LHC) for particle physicists
   - Full-text search

** Query Languages
   - Declarative
     - Hides implementation details of the database engine
   - Imperative

*** MapReduce in MongoDB
    #+begin_src js
      db.users.mapReduce(
          function map() {
              var age = this.age;
              emit(age, this.height); // emit(key, value)
          },
          function reduce(key, values) {
              return Array.avg(values);
          },
          {
              query: { gender: "male"},
              out: "AvgHeightReport"
          }
      );
    #+end_src
    - ~map~ and ~reduce~ functions should be pure functions (no side effect). This allow the database to run the functions anywhere, in any order, and rerun them on failure.


** Summary
   - Document databases target use cases where data comes in self-contained documents and relationships between one document and another are *rare*.
   - Graph databases go in the opposite direction, targeting use cases where anything is potentially related to everything.


* Tools
  - Redis
    - datastore also used as message queue
  - Kafka
    - message queue with database-like *durability guarantees*.
