#+TITLE: Linear Algebra
#+AUTHOR: chrischen
#+EMAIL: chrischen3121@gmail.com
#+OPTIONS: H:3 toc:2 num:2 ^:{}
* Vector
** Addition
   - Commutative: $\vec{a}+\vec{b}=\vec{b}+\vec{a}$
   - Associativity: $(\vec{a}+\vec{b}) + \vec{c}=\vec{a} + (\vec{b} + \vec{c})$
   - Additive Identity: There exists an element $\vec{0}$ such that $\vec{a}+\vec{0}=\vec{a}$
   - Additive Inverse: $\forall{\vec{a}}\exists{\vec{b}}\vec{a}+\vec{b}=\vec{0}$
   - Multiplicative Identity: $\forall{\vec{a}}\vec{1}\vec{a}=\vec{a}$
   - Distributive: $s(\vec{a}+\vec{b})=s\vec{a}+s\vec{b}$

** Magnitudes(Modulus)
   $$\vec{a} = \begin{bmatrix}x \\ y\end{bmatrix}$$
   $$\left\|\vec{a}\right\|=\sqrt{x^2+y^2}$$
   - $\left\|\vec{a}\right\|$ is the length of a vector.

** Multiplication(Dot Product)
*** Geometric Meaning
    - Cosine Rule: $c^2=a^2+b^2-2ab\cos{\theta}$
      $$\left\|\vec{x}-\vec{y}\right\|^2=\left\|\vec{x}\right\|^2 + \left\|\vec{y}\right\|^2 - 2\left\|\vec{x}\right\|\left\|\vec{y}\right\|\cos{\theta}$$
      $$\because\left\|\vec{x}\right\|^2 = \vec{x}\cdot\vec{x}\mathbf{\ and\ }(\vec{x}-\vec{y})\cdot(\vec{x}-\vec{y})=\vec{x}\cdot\vec{x}+\vec{y}\cdot\vec{y}-2\vec{x}\cdot\vec{y} \mathbf{\ Distributive Rule}$$
      $$\vec{x}\cdot\vec{y}=\left\|\vec{x}\right\|\times\mathbf{projection\ y\ on\ x}=\left\|\vec{x}\right\|\left\|\vec{y}\right\|\cos{\theta}$$
      $$\frac{\vec{x}\cdot\vec{y}}{\left\|\vec{x}\right\|}=\left\|\vec{y}\right\|\cos\theta \text{(scaler projection)}$$
    - $\left\|\vec{y}\right\|\cos{\theta}$ is the *scaler projection* of $\vec{y}$ on $\vec{x}$
    - Dot product is also called *projection product*.

*** Vector Projection
    [[../resources/math/LinearAlgebra/scale_projection.png]]
    - The *scaler projection* is the size of the green vector.
      - $\frac{\vec{s}\cdot\vec{r}}{\left\|\vec{r}\right\|}$
    - The *vector projection* is the green vector and can be calculated
      $$\mathbf{vector\ projection}=\mathbf{scaler\ projection}\times\frac{\vec{r}}{\left\|\vec{r}\right\|}=\frac{\vec{s}\cdot\vec{r}}{\left\|\vec{r}\right\|}\times\frac{\vec{r}}{\left\|\vec{r}\right\|}=\frac{\vec{s}\cdot\vec{r}}{\vec{r}\cdot\vec{r}}\cdot \vec{r}$$
      - $\frac{\vec{r}}{\left\|\vec{r}\right\|}$ is the unit length of $\vec{r}$
      - $\frac{\vec{s}\cdot\vec{r}}{\vec{r}\cdot\vec{r}}$ is the new coordinate of $\vec{s}$ on $\vec{r}$

*** Definition
    $$\vec{x}\cdot\vec{y}=\sum_{i=1}^n x_i y_i$$

*** Rules
   - Commutative
   - Distributive over Addition: $\vec{a}(\vec{b}+\vec{c})=\vec{a}\vec{b}+\vec{a}\vec{c}$
   - $\vec{a}\cdot(s\vec{b})=s(\vec{a}\cdot\vec{b})$
   - $\vec{a}\cdot\vec{a}=\left\|\vec{a}\right\|^2$

* Matrix
  Matrices make transformations on vectors, potentially changing their magnitude and direction.

  - Matrix Multiplication isn't commutative, but associative.
    $$A\cdot B \ne B\cdot A$$
    $$A\cdot (B \cdot C) = (A\cdot B) \cdot C$$

** Rotation Matrix
   - 2D
   $$\begin{bmatrix}
   \cos\theta & \sin\theta \\
   -\sin\theta & \cos\theta
   \end{bmatrix}$$
   - 3D
   $$\begin{bmatrix}
   \cos\theta & \sin\theta & 0 \\
   -\sin\theta & \cos\theta & 0 \\
   0 & 0 & 1
   \end{bmatrix}$$

** Matrix Inverses
   $$A^{-1}\cdot A=I$$
   $$A\cdot \mathbf{r} = \mathbf{s}$$
   $$A^{-1}\cdot A\cdot \mathbf{r}=A^{-1}\cdot\mathbf{s}$$
   $$I\mathbf{r}=A^{-1}\cdot\mathbf{s}$$
   - our goal is to find $A^{-1}$

*** Singular
    If a matrix is singular, that means an inverse doesn't exist.

** Python Code
   #+begin_src python
     import numpy as np

     A = [[1, 1, 3],
          [1, 2, 4],
          [1, 1, 2]]

     Avin = np.linalg.inv(A)
     Avin.dot(A) # => Identity Matrix

     s = [5, 8, 9]
     r = np.linalg.solve(A, s) # => array([10.,  7., -4.])
   #+end_src
** Dot Product
   - Einstein's summation notation for $\mathbf{C=AB}$
     $$c_{ik}=a_{ij} b_{jk}$$
   - Vector: $\mathbf{r'=Ar}$
     $$r^\prime_i=A_{ij}r_j$$
   - Matrix: $\mathbf{R'=AR}$
     $$R^\prime_{ia}=A_{ij}R_{ja}$$
* Basis Transformation
  $$\mathbf{B}\cdot\begin{bmatrix} x_{b^\prime} \\ y_{b^\prime} \end{bmatrix}=\begin{bmatrix} x_b \\ y_b \end{bmatrix}$$
  $$\mathbf{B^{-1}}\cdot \begin{bmatrix} x_b \\ y_b \end{bmatrix}=\begin{bmatrix} x_{b^\prime} \\ y_{b^\prime} \end{bmatrix}$$
  - $B$ is the new basis written in old basis language.
  - $B^{-1}$ is the new old written in new basis language.
